services:
  litellm:
    image: ghcr.io/berriai/litellm:main
    ports:
      - "4000:4000"
    environment:
      # Provide ONE of these (or configure litellm.yaml). For single-user local, env vars are simplest.
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      # Optional: set a proxy master key. If set, also set LITELLM_API_KEY in the web service.
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      # Enable Ollama routing in litellm.yaml
      - OLLAMA_API_BASE=http://ollama:11434
    volumes:
      - ./litellm.yaml:/app/config.yaml:ro
    command: ["--port", "4000", "--host", "0.0.0.0"]
    restart: unless-stopped

  # Local LLM runtime (alternative to OpenAI/Claude). Pull model after first start:
  #   docker exec -it <ollama-container> ollama pull llama3.3
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    restart: unless-stopped

  # Optional Stable Diffusion sidecar (ComfyUI) for map/art image generation.
  # NOTE: This repo does not yet call ComfyUI automatically; this is a sidecar you can run alongside.
  # With ~8GB VRAM, prefer SD1.5/SDXL-turbo style models and keep resolution modest.
  comfyui:
    image: ghcr.io/comfyanonymous/comfyui:latest
    profiles: ["sd"]
    ports:
      - "8188:8188"
    volumes:
      - ./models/comfyui:/app/models
      - ./data/comfyui:/app/output
    restart: unless-stopped

  web:
    build: .
    ports:
      - "8000:8000"
    environment:
      - LITELLM_BASE_URL=http://litellm:4000
      - LITELLM_API_KEY=${LITELLM_MASTER_KEY}
      - DND_DEFAULT_MODEL=${DND_DEFAULT_MODEL}
    volumes:
      - ./data:/app/data
    depends_on:
      - litellm

volumes:
  ollama:
